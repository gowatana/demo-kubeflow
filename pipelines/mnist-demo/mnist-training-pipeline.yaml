apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: mnist-training-and-prediction-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.3, pipelines.kubeflow.org/pipeline_compilation_time: '2024-02-19T10:35:08.387599',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "A pipeline that trains
      an MNIST model, saves it to PVC and makes a prediction.", "inputs": [{"default":
      "demo-vol-01", "name": "pvc_name", "optional": true}], "name": "Mnist Training
      and Prediction Pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.3}
spec:
  entrypoint: mnist-training-and-prediction-pipeline
  templates:
  - name: mnist-training-and-prediction-pipeline
    inputs:
      parameters:
      - {name: pvc_name}
    dag:
      tasks:
      - name: preproc-dataset
        template: preproc-dataset
        dependencies: [save-dataset]
        arguments:
          parameters:
          - {name: pvc_name, value: '{{inputs.parameters.pvc_name}}'}
      - name: save-dataset
        template: save-dataset
        arguments:
          parameters:
          - {name: pvc_name, value: '{{inputs.parameters.pvc_name}}'}
      - name: test-model
        template: test-model
        dependencies: [train-and-save-model]
        arguments:
          parameters:
          - {name: pvc_name, value: '{{inputs.parameters.pvc_name}}'}
      - name: train-and-save-model
        template: train-and-save-model
        dependencies: [preproc-dataset]
        arguments:
          parameters:
          - {name: pvc_name, value: '{{inputs.parameters.pvc_name}}'}
  - name: preproc-dataset
    container:
      args: []
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def preproc_dataset():
            import tensorflow as tf
            import numpy as np
            import os

            load_dir = '/mnt/data/dataset'
            save_dir = '/mnt/data/preproc_dataset'
            if not os.path.exists(save_dir):
                os.makedirs(save_dir)

            with np.load(os.path.join(load_dir, 'mnist.npz'), allow_pickle=True) as data:
                x_train = data['x_train']
                y_train = data['y_train']
                x_test = data['x_test']
                y_test = data['y_test']

            x_train, x_test = x_train / 255.0, x_test / 255.0
            np.savez_compressed(os.path.join(save_dir, 'mnist_normalized.npz'), x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)

        import argparse
        _parser = argparse.ArgumentParser(prog='Preproc dataset', description='')
        _parsed_args = vars(_parser.parse_args())

        _outputs = preproc_dataset(**_parsed_args)
      image: tensorflow/tensorflow:latest
      volumeMounts:
      - {mountPath: /mnt/data, name: pvolume-e819e39f92a888af338a32082ea85feaa488e4a0aabfacc858d718b}
    inputs:
      parameters:
      - {name: pvc_name}
    metadata:
      labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.3, pipelines.kubeflow.org/pipeline-sdk-type: kfp}
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\"
          \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          preproc_dataset():\n    import tensorflow as tf\n    import numpy as np\n    import
          os\n\n    load_dir = ''/mnt/data/dataset''\n    save_dir = ''/mnt/data/preproc_dataset''\n    if
          not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    with
          np.load(os.path.join(load_dir, ''mnist.npz''), allow_pickle=True) as data:\n        x_train
          = data[''x_train'']\n        y_train = data[''y_train'']\n        x_test
          = data[''x_test'']\n        y_test = data[''y_test'']\n\n    x_train, x_test
          = x_train / 255.0, x_test / 255.0\n    np.savez_compressed(os.path.join(save_dir,
          ''mnist_normalized.npz''), x_train=x_train, y_train=y_train, x_test=x_test,
          y_test=y_test)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Preproc
          dataset'', description='''')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = preproc_dataset(**_parsed_args)\n"], "image": "tensorflow/tensorflow:latest"}},
          "name": "Preproc dataset"}', pipelines.kubeflow.org/component_ref: '{}'}
    volumes:
    - name: pvolume-e819e39f92a888af338a32082ea85feaa488e4a0aabfacc858d718b
      persistentVolumeClaim: {claimName: '{{inputs.parameters.pvc_name}}'}
  - name: save-dataset
    container:
      args: []
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def save_dataset():
            import tensorflow as tf
            import numpy as np
            import os

            save_dir = '/mnt/data/dataset'
            if not os.path.exists(save_dir):
                os.makedirs(save_dir)

            mnist = tf.keras.datasets.mnist
            (x_train, y_train), (x_test, y_test) = mnist.load_data()

            np.savez_compressed(os.path.join(save_dir, 'mnist.npz'), x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)

        import argparse
        _parser = argparse.ArgumentParser(prog='Save dataset', description='')
        _parsed_args = vars(_parser.parse_args())

        _outputs = save_dataset(**_parsed_args)
      image: tensorflow/tensorflow:latest
      volumeMounts:
      - {mountPath: /mnt/data, name: pvolume-e819e39f92a888af338a32082ea85feaa488e4a0aabfacc858d718b}
    inputs:
      parameters:
      - {name: pvc_name}
    metadata:
      labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.3, pipelines.kubeflow.org/pipeline-sdk-type: kfp}
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\"
          \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          save_dataset():\n    import tensorflow as tf\n    import numpy as np\n    import
          os\n\n    save_dir = ''/mnt/data/dataset''\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    mnist
          = tf.keras.datasets.mnist\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n    np.savez_compressed(os.path.join(save_dir,
          ''mnist.npz''), x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Save dataset'', description='''')\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = save_dataset(**_parsed_args)\n"],
          "image": "tensorflow/tensorflow:latest"}}, "name": "Save dataset"}', pipelines.kubeflow.org/component_ref: '{}'}
    volumes:
    - name: pvolume-e819e39f92a888af338a32082ea85feaa488e4a0aabfacc858d718b
      persistentVolumeClaim: {claimName: '{{inputs.parameters.pvc_name}}'}
  - name: test-model
    container:
      args: []
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def test_model():
            import tensorflow as tf
            import numpy as np
            import os

            data_load_dir = '/mnt/data/preproc_dataset'
            model_load_dir = '/mnt/data/saved_model'
            with np.load(os.path.join(data_load_dir, 'mnist_normalized.npz'), allow_pickle=True) as data:
                x_test = data['x_test']
                y_test = data['y_test']

            model = tf.keras.models.load_model(model_load_dir)
            loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
            model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])
            model.evaluate(x_test, y_test, verbose=2)

        import argparse
        _parser = argparse.ArgumentParser(prog='Test model', description='')
        _parsed_args = vars(_parser.parse_args())

        _outputs = test_model(**_parsed_args)
      image: tensorflow/tensorflow:latest
      volumeMounts:
      - {mountPath: /mnt/data, name: pvolume-e819e39f92a888af338a32082ea85feaa488e4a0aabfacc858d718b}
    inputs:
      parameters:
      - {name: pvc_name}
    metadata:
      labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.3, pipelines.kubeflow.org/pipeline-sdk-type: kfp}
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\"
          \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          test_model():\n    import tensorflow as tf\n    import numpy as np\n    import
          os\n\n    data_load_dir = ''/mnt/data/preproc_dataset''\n    model_load_dir
          = ''/mnt/data/saved_model''\n    with np.load(os.path.join(data_load_dir,
          ''mnist_normalized.npz''), allow_pickle=True) as data:\n        x_test =
          data[''x_test'']\n        y_test = data[''y_test'']\n\n    model = tf.keras.models.load_model(model_load_dir)\n    loss_fn
          = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    model.compile(optimizer=''adam'',
          loss=loss_fn, metrics=[''accuracy''])\n    model.evaluate(x_test, y_test,
          verbose=2)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Test
          model'', description='''')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = test_model(**_parsed_args)\n"], "image": "tensorflow/tensorflow:latest"}},
          "name": "Test model"}', pipelines.kubeflow.org/component_ref: '{}'}
    volumes:
    - name: pvolume-e819e39f92a888af338a32082ea85feaa488e4a0aabfacc858d718b
      persistentVolumeClaim: {claimName: '{{inputs.parameters.pvc_name}}'}
  - name: train-and-save-model
    container:
      args: []
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def train_and_save_model():
            import tensorflow as tf
            import numpy as np
            import os

            load_dir = '/mnt/data/preproc_dataset'
            save_dir = '/mnt/data/saved_model'
            if not os.path.exists(save_dir):
                os.makedirs(save_dir)

            with np.load(os.path.join(load_dir, 'mnist_normalized.npz'), allow_pickle=True) as data:
                x_train = data['x_train']
                y_train = data['y_train']
                x_test = data['x_test']
                y_test = data['y_test']

            model = tf.keras.models.Sequential([
                tf.keras.layers.Flatten(input_shape=(28, 28)),
                tf.keras.layers.Dense(128, activation='relu'),
                tf.keras.layers.Dropout(0.2),
                tf.keras.layers.Dense(10)
            ])
            loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
            model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])
            model.fit(x_train, y_train, epochs=5)
            model.save(save_dir)

        import argparse
        _parser = argparse.ArgumentParser(prog='Train and save model', description='')
        _parsed_args = vars(_parser.parse_args())

        _outputs = train_and_save_model(**_parsed_args)
      image: tensorflow/tensorflow:latest
      volumeMounts:
      - {mountPath: /mnt/data, name: pvolume-e819e39f92a888af338a32082ea85feaa488e4a0aabfacc858d718b}
    inputs:
      parameters:
      - {name: pvc_name}
    metadata:
      labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.3, pipelines.kubeflow.org/pipeline-sdk-type: kfp}
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\"
          \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          train_and_save_model():\n    import tensorflow as tf\n    import numpy as
          np\n    import os\n\n    load_dir = ''/mnt/data/preproc_dataset''\n    save_dir
          = ''/mnt/data/saved_model''\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    with
          np.load(os.path.join(load_dir, ''mnist_normalized.npz''), allow_pickle=True)
          as data:\n        x_train = data[''x_train'']\n        y_train = data[''y_train'']\n        x_test
          = data[''x_test'']\n        y_test = data[''y_test'']\n\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Flatten(input_shape=(28,
          28)),\n        tf.keras.layers.Dense(128, activation=''relu''),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(10)\n    ])\n    loss_fn
          = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    model.compile(optimizer=''adam'',
          loss=loss_fn, metrics=[''accuracy''])\n    model.fit(x_train, y_train, epochs=5)\n    model.save(save_dir)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Train and save model'',
          description='''')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = train_and_save_model(**_parsed_args)\n"], "image": "tensorflow/tensorflow:latest"}},
          "name": "Train and save model"}', pipelines.kubeflow.org/component_ref: '{}'}
    volumes:
    - name: pvolume-e819e39f92a888af338a32082ea85feaa488e4a0aabfacc858d718b
      persistentVolumeClaim: {claimName: '{{inputs.parameters.pvc_name}}'}
  arguments:
    parameters:
    - {name: pvc_name, value: demo-vol-01}
  serviceAccountName: pipeline-runner
