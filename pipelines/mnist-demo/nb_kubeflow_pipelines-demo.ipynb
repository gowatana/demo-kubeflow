{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "-PqeusNlukFs",
   "metadata": {
    "id": "-PqeusNlukFs"
   },
   "source": [
    "TensorFlowを利用したMNISTのトレーニングを、Kubeflow Pipelinesに実装してみる。\n",
    "\n",
    "実行環境\n",
    "* Kubeflow 1.6.1\n",
    "* Notebook: kubeflownotebookswg/jupyter-tensorflow-full:v1.6.0\n",
    "* Image:\n",
    "  * tensorflow/tensorflow:latest\n",
    "  * kubeflowmnist2024001.azurecr.io/mnist-demo/mnist-inference:v1\n",
    "  * ACR Secret が必要"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iENvRY83y6ET",
   "metadata": {
    "id": "iENvRY83y6ET"
   },
   "source": [
    "# 事前準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xNLaPLuxzE28",
   "metadata": {
    "id": "xNLaPLuxzE28"
   },
   "source": [
    "## KubeflowでNotebookを作成する\n",
    "* tensorflow, kfp などは、Notebookのコンテナ イメージ（jupyter-tensorflow-full:v1.6.0）に含まれている。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eIquLEsuNNi",
   "metadata": {
    "id": "3eIquLEsuNNi"
   },
   "source": [
    "## PVCの準備\n",
    "PVCを作成してあることを確認する。\n",
    "PVCは、Kubeflow UIのVolumesメニューから作成しておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b57b81f-db63-4a81-85ec-7087969c603f",
   "metadata": {
    "id": "4b57b81f-db63-4a81-85ec-7087969c603f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\n",
      "demo-vol-01   Bound    pvc-1d664eec-7630-4164-ae76-823dff2ef304   3Gi        RWO            default        54m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pvc demo-vol-01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JbrQFtoIw1Q5",
   "metadata": {
    "id": "JbrQFtoIw1Q5"
   },
   "source": [
    "コンパイルで生成されたYAMLを確認する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6JpC_jZhw5vR",
   "metadata": {
    "id": "6JpC_jZhw5vR"
   },
   "outputs": [],
   "source": [
    "!cat mnist_pipeline_and_predict.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a739a5b6",
   "metadata": {},
   "source": [
    "# パインプラインの作成（分割版）\n",
    "\n",
    "トレーニングと推論のコードを、それぞれNGCのTensorFlowイメージでコンテナ化して利用する。\n",
    "\n",
    "* コンテナは ACR に配置ずみ。（ServiceAccount default-editorに、ImagePullSecretが必要）\n",
    "* PVC demo-vol-01 が必要"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3da23e",
   "metadata": {},
   "source": [
    "## 学習パイプライン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fe4d93c-6804-4f39-9c58-34cde053a145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as comp\n",
    "#import tensorflow as tf\n",
    "#import numpy as np\n",
    "\n",
    "def save_dataset():\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    save_dir = '/mnt/data/dataset'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    np.savez_compressed(os.path.join(save_dir, 'mnist.npz'), x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)\n",
    "    \n",
    "def preproc_dataset():\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    load_dir = '/mnt/data/dataset'\n",
    "    save_dir = '/mnt/data/preproc_dataset'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    with np.load(os.path.join(load_dir, 'mnist.npz'), allow_pickle=True) as data:\n",
    "        x_train = data['x_train']\n",
    "        y_train = data['y_train']\n",
    "        x_test = data['x_test']\n",
    "        y_test = data['y_test']\n",
    "\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    np.savez_compressed(os.path.join(save_dir, 'mnist_normalized.npz'), x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)\n",
    "\n",
    "def train_and_save_model():\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    load_dir = '/mnt/data/preproc_dataset'\n",
    "    save_dir = '/mnt/data/saved_model'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    with np.load(os.path.join(load_dir, 'mnist_normalized.npz'), allow_pickle=True) as data:\n",
    "        x_train = data['x_train']\n",
    "        y_train = data['y_train']\n",
    "        x_test = data['x_test']\n",
    "        y_test = data['y_test']\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10)\n",
    "    ])\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=5)\n",
    "    model.save(save_dir)\n",
    "\n",
    "def test_model():\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    data_load_dir = '/mnt/data/preproc_dataset'\n",
    "    model_load_dir = '/mnt/data/saved_model'\n",
    "    with np.load(os.path.join(data_load_dir, 'mnist_normalized.npz'), allow_pickle=True) as data:\n",
    "        x_test = data['x_test']\n",
    "        y_test = data['y_test']\n",
    "\n",
    "    model = tf.keras.models.load_model(model_load_dir)\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "    model.evaluate(x_test, y_test, verbose=2)\n",
    "    \n",
    "def inference(pvc_name):\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    model = tf.keras.models.load_model(f'/mnt/data/saved_model')\n",
    "    new_data = np.random.rand(28, 28)\n",
    "    new_data = new_data.reshape(1, 28, 28) / 255.0\n",
    "    predictions = model.predict(new_data)\n",
    "    predicted_class = np.argmax(predictions, axis=1)\n",
    "    print(\"Predicted class:\", predicted_class)\n",
    "    with open(f'/mnt/data/predictions.txt', 'w') as f:\n",
    "        f.write(\"Predicted class: \" + str(predicted_class[0]) + \"\\n\")\n",
    "\n",
    "# コンテナオペレーションを作成\n",
    "save_dataset_op = comp.func_to_container_op(save_dataset, base_image='tensorflow/tensorflow:latest')\n",
    "preproc_dataset_op = comp.func_to_container_op(preproc_dataset, base_image='tensorflow/tensorflow:latest')\n",
    "train_and_save_op = comp.func_to_container_op(train_and_save_model, base_image='tensorflow/tensorflow:latest')\n",
    "test_model_op = comp.func_to_container_op(test_model, base_image='tensorflow/tensorflow:latest')\n",
    "#inference_op = comp.func_to_container_op(inference, base_image='tensorflow/tensorflow:latest')\n",
    "\n",
    "# パイプラインに推論タスクを追加\n",
    "@dsl.pipeline(\n",
    "    name='Mnist Training and Prediction Pipeline',\n",
    "    description='A pipeline that trains an MNIST model, saves it to PVC and makes a prediction.'\n",
    ")\n",
    "\n",
    "def mnist_pipeline(pvc_name='demo-vol-01'):\n",
    "    save_dataset_task = save_dataset_op()\n",
    "    save_dataset_task.add_pvolumes({f'/mnt/data': dsl.PipelineVolume(pvc=pvc_name)})\n",
    "    \n",
    "    preproc_dataset_task = preproc_dataset_op()\n",
    "    preproc_dataset_task.add_pvolumes({f'/mnt/data': dsl.PipelineVolume(pvc=pvc_name)})\n",
    "    preproc_dataset_task.after(save_dataset_task)\n",
    "    \n",
    "    train_and_save_task = train_and_save_op()\n",
    "    train_and_save_task.add_pvolumes({f'/mnt/data': dsl.PipelineVolume(pvc=pvc_name)})\n",
    "    train_and_save_task.after(preproc_dataset_task)\n",
    "\n",
    "    test_model_task = test_model_op()\n",
    "    test_model_task.add_pvolumes({f'/mnt/data': dsl.PipelineVolume(pvc=pvc_name)})\n",
    "    test_model_task.after(train_and_save_task)\n",
    "    \n",
    "    #inference_task = inference_op(pvc_name)\n",
    "    #inference_task.add_pvolumes({f'/mnt/data': dsl.PipelineVolume(pvc=pvc_name)})\n",
    "    #inference_task.after(test_model_task)\n",
    "    \n",
    "# コンパイル\n",
    "kfp.compiler.Compiler().compile(mnist_pipeline, 'mnist-training-pipeline.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8GWzEvhmiDw",
   "metadata": {
    "id": "e8GWzEvhmiDw"
   },
   "source": [
    "YAMLが生成されたことを確認する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ra3dp460mlJe",
   "metadata": {
    "id": "Ra3dp460mlJe"
   },
   "outputs": [],
   "source": [
    "!ls -l mnist-training-pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pk2p13wympBo",
   "metadata": {
    "id": "pk2p13wympBo"
   },
   "source": [
    "YAMLファイルは、Kubeflow PipelinesのUIからアップロードする。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2732814a",
   "metadata": {},
   "source": [
    "## 推論パイプライン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3fd31022-330b-4250-9557-ee4dabb2d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論パイプライン\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as comp\n",
    "#import tensorflow as tf\n",
    "#import numpy as np\n",
    "\n",
    "def save_dataset_png():\n",
    "    from PIL import Image\n",
    "    import tensorflow as tf\n",
    "    import os\n",
    "\n",
    "    dir_path = '/mnt/data/images'\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    for i in range(10):\n",
    "        img = Image.fromarray(x_train[i])\n",
    "        img.save(os.path.join(dir_path, f'test_{i}.png'))\n",
    "        \n",
    "def inference(pvc_name, image_file_path):       \n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    model_dir_path = '/mnt/data/saved_model'\n",
    "    \n",
    "    img = Image.open(image_file_path)\n",
    "    img = img.convert('L').resize((28, 28))\n",
    "    img_array = np.array(img) / 255.0\n",
    "    img_array = img_array.reshape(1, 28, 28)\n",
    "\n",
    "    model = tf.keras.models.load_model(model_dir_path)\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = np.argmax(predictions, axis=1)\n",
    "    with open(f'/mnt/data/inference-number.txt', 'w') as f:\n",
    "        f.write(\"Inference Number: \" + str(predicted_class[0]) + \"\\n\")\n",
    "\n",
    "# コンテナオペレーションを作成\n",
    "save_dataset_png_op = comp.func_to_container_op(save_dataset_png, base_image='kubeflowmnist2024001.azurecr.io/mnist-demo/mnist-inference:v1')\n",
    "inference_op = comp.func_to_container_op(inference, base_image='kubeflowmnist2024001.azurecr.io/mnist-demo/mnist-inference:v1')\n",
    "\n",
    "# パイプラインに推論タスクを追加\n",
    "@dsl.pipeline(\n",
    "    name='Mnist Inference Pipeline',\n",
    "    description='Inference pipeline with an MNIST model.'\n",
    ")\n",
    "\n",
    "def mnist_pipeline(pvc_name='demo-vol-01', image_file_path = '/mnt/data/images/test_0.png'):\n",
    "    save_dataset_png_task = save_dataset_png_op()\n",
    "    save_dataset_png_task.add_pvolumes({f'/mnt/data': dsl.PipelineVolume(pvc=pvc_name)})\n",
    "    \n",
    "    inference_task = inference_op(pvc_name, image_file_path)\n",
    "    inference_task.add_pvolumes({f'/mnt/data': dsl.PipelineVolume(pvc=pvc_name)})\n",
    "    inference_task.after(save_dataset_png_task)\n",
    "    \n",
    "# コンパイル\n",
    "kfp.compiler.Compiler().compile(mnist_pipeline, 'mnist-inference-pipeline.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb96e995",
   "metadata": {},
   "source": [
    "## 推論パイプライン（Output 修正版）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "98329640-1f8c-4c46-86d2-0dc13764af72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output修正\n",
    "# 推論パイプライン\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as comp\n",
    "#import tensorflow as tf\n",
    "#import numpy as np\n",
    "\n",
    "def save_dataset_png():\n",
    "    from PIL import Image\n",
    "    import tensorflow as tf\n",
    "    import os\n",
    "\n",
    "    dir_path = '/mnt/data/images'\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    for i in range(10):\n",
    "        img = Image.fromarray(x_train[i])\n",
    "        img.save(os.path.join(dir_path, f'test_{i}.png'))\n",
    "        \n",
    "def inference(pvc_name, image_file_path) -> str:\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    model_dir_path = '/mnt/data/saved_model'\n",
    "    \n",
    "    img = Image.open(image_file_path)\n",
    "    img = img.convert('L').resize((28, 28))\n",
    "    img_array = np.array(img) / 255.0\n",
    "    img_array = img_array.reshape(1, 28, 28)\n",
    "\n",
    "    model = tf.keras.models.load_model(model_dir_path)\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    output_file = '/mnt/data/inference-number.txt'\n",
    "    with open(output_file, 'w') as f:\n",
    "        inference_output = \"Inference Number: \" + str(predicted_class[0]) + \"\\n\"\n",
    "        f.write(inference_output)\n",
    "    return inference_output\n",
    "\n",
    "# コンテナオペレーションを作成\n",
    "save_dataset_png_op = comp.func_to_container_op(save_dataset_png, base_image='kubeflowmnist2024001.azurecr.io/mnist-demo/mnist-inference:v1')\n",
    "inference_op = comp.func_to_container_op(inference, base_image='kubeflowmnist2024001.azurecr.io/mnist-demo/mnist-inference:v1')\n",
    "\n",
    "# パイプラインに推論タスクを追加\n",
    "@dsl.pipeline(\n",
    "    name='Mnist Inference Pipeline',\n",
    "    description='Inference pipeline with an MNIST model.'\n",
    ")\n",
    "\n",
    "def mnist_pipeline(pvc_name='demo-vol-01', image_file_path = '/mnt/data/images/test_0.png'):\n",
    "    save_dataset_png_task = save_dataset_png_op()\n",
    "    save_dataset_png_task.add_pvolumes({f'/mnt/data': dsl.PipelineVolume(pvc=pvc_name)})\n",
    "    \n",
    "    inference_task = inference_op(pvc_name, image_file_path)\n",
    "    inference_task.add_pvolumes({f'/mnt/data': dsl.PipelineVolume(pvc=pvc_name)})\n",
    "    inference_task.after(save_dataset_png_task)\n",
    "    \n",
    "# コンパイル\n",
    "kfp.compiler.Compiler().compile(mnist_pipeline, 'mnist-inference-pipeline.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2LDWpQ3pvG-j",
   "metadata": {
    "id": "2LDWpQ3pvG-j"
   },
   "source": [
    "# パイプラインの作成（YAML のアップロード）\n",
    "\n",
    "生成されたYAMLはローカルにダウンロードして、Kubeflow UIの「Pipelines (KFP)」からアップロードする。\n",
    "* ブラウザで、Kubeflow UI を開く\n",
    "* Pipelines 画面を開く\n",
    "* 「Upload Pipeline」からアップロードする"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oDv0gsp9yc7X",
   "metadata": {
    "id": "oDv0gsp9yc7X"
   },
   "source": [
    "# パイプラインの実行\n",
    "\n",
    "パイプラインを実行する。\n",
    "* Experimentsの作成（例：ex-01）\n",
    "* 「Create Run」で、パイプラインを実行する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XEFjOutesjIY",
   "metadata": {
    "id": "XEFjOutesjIY",
    "tags": []
   },
   "source": [
    "# PVに保存されたデータの確認\n",
    "\n",
    "PVをマウントしたPodを起動して、保存されたデータを確認する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YW1Zf7vMsyMo",
   "metadata": {
    "id": "YW1Zf7vMsyMo"
   },
   "outputs": [],
   "source": [
    "!kubectl apply -f pod.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uXKU4Qits4Au",
   "metadata": {
    "id": "uXKU4Qits4Au"
   },
   "outputs": [],
   "source": [
    "!kubectl get pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ogoPaOJstTCv",
   "metadata": {
    "id": "ogoPaOJstTCv"
   },
   "outputs": [],
   "source": [
    "!kubectl exec demo-pod  -- df /mnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h_XIJ_0ps-lA",
   "metadata": {
    "id": "h_XIJ_0ps-lA"
   },
   "outputs": [],
   "source": [
    "!kubectl exec demo-pod -- ls /mnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ZD01dhtbx8",
   "metadata": {
    "id": "27ZD01dhtbx8"
   },
   "outputs": [],
   "source": [
    "!kubectl exec demo-pod  -- cat /mnt/data/inference-number.txt"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
