{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33aac63a",
   "metadata": {},
   "source": [
    "# LLM でのテキスト生成\n",
    "\n",
    "* OpenCALM-small のモデルをダウンロード → テキスト生成\n",
    "\n",
    "注意点：\n",
    "* 超軽量モデルなので、生成されるテキストは微妙。かわりにCPUでもそこそこ動作する。\n",
    "* モデル自体の日本語入力は可能だが、Kubeflow UIのinput_textでの入力は英語のみ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ff2996",
   "metadata": {},
   "source": [
    "## transformersとPyTorchのバージョン確認とインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34d222d4-877a-463d-859e-5141afaad3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.37.2'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c077004a-4240-464a-8db0-b7b6b2383807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0+cu121'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d743b6-cbf5-44cc-81c2-4625936bcf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffa8daf",
   "metadata": {},
   "source": [
    "## モデルの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "892d604f-f4c5-4178-88b7-1e8694fd0aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの保存\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_save_path = \"./trained_model\"\n",
    "model_name = \"cyberagent/open-calm-small\"\n",
    "\n",
    "# モデルとトークナイザーのロード\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# モデルとトークナイザーの保存\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "model.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded4bbed",
   "metadata": {},
   "source": [
    "## テキストの生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69b9e953-0484-4937-9730-c98854a3702e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text 1: VMware Private AIは我々のビジネスに、そして未来に向けて本当に役立つものです。\n",
      "また、このプロダクトを通じて得た経験により、「Deep Lear\n",
      "Generated text 2: VMware Private AIは我々のビジネスに、世界を変えるかもしれない。\n",
      "「Personal Computing」とはすなわち、「コンピューティング」「クラウド・データセンター\n"
     ]
    }
   ],
   "source": [
    "# テキスト生成\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline #, set_seed\n",
    "\n",
    "model_save_path = \"./trained_model\"\n",
    "\n",
    "# GPUが利用可能かどうかを確認し、利用可能な場合はそのインデックスを、そうでない場合はCPUを使用する\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_save_path, token=False, truncation=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_save_path, token=False)\n",
    "\n",
    "# 生成用のパイプラインをセットアップ\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "# 生成する文章のシード値を設定（オプション）\n",
    "#set_seed(42)\n",
    "\n",
    "# テキスト生成\n",
    "input_text = \"VMware Private AIは我々のビジネスに、\"\n",
    "generated_texts = generator(\n",
    "    input_text,\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.2,\n",
    "    top_k=50,\n",
    "    pad_token_id=0\n",
    "    )\n",
    "\n",
    "for i, text in enumerate(generated_texts):\n",
    "    print(f\"Generated text {i+1}: {text['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9b0e96",
   "metadata": {},
   "source": [
    "# パイプライン YAML の作成（モデル保存のみ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bcb13fa-6e0d-4d76-b0a6-15319e201da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as comp\n",
    "\n",
    "# コンポーネントをYAMLファイルからロード\n",
    "save_op = kfp.components.load_component_from_file('save_model.yaml')\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='Save LLM Model Pipeline',\n",
    "    description='Pipeline for saving an LLM model to PV.'\n",
    ")\n",
    "\n",
    "def mnist_pipeline(\n",
    "        pvc_name: str = 'demo-vol-02',\n",
    "        model_name: str = 'cyberagent/open-calm-small'\n",
    "    ):  # PVCとModelの名前をパラメータとして追加\n",
    "    \n",
    "    # トレーニングタスク\n",
    "    save_task = save_op(model_name=model_name)\n",
    "    save_task.add_pvolumes({'/mnt/save_model': dsl.PipelineVolume(pvc=pvc_name)})\n",
    "\n",
    "# コンパイル\n",
    "kfp.compiler.Compiler().compile(mnist_pipeline, 'llm_pipeline_save-model.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd05a97b",
   "metadata": {},
   "source": [
    "# パイプライン YAML の作成（モデル保存 → テキスト生成）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4c501ce-3abb-4343-9cbf-23ea3a490ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as comp\n",
    "\n",
    "# コンポーネントをYAMLファイルからロード\n",
    "save_op = kfp.components.load_component_from_file('save_model.yaml')\n",
    "gen_text_op = kfp.components.load_component_from_file('gen_text.yaml')\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='Save LLM Model and Generate Text Pipeline',\n",
    "    description='Pipeline for saving an LLM model to PV, and Generate Text.'\n",
    ")\n",
    "\n",
    "def mnist_pipeline(\n",
    "        model_pvc_name: str = 'demo-vol-02',\n",
    "        output_pvc_name: str = 'demo-vol-03',\n",
    "        model_name: str = 'cyberagent/open-calm-small',\n",
    "        input_text: str = 'VMware Private AI is'\n",
    "    ):  # PVCとModelの名前、入力テキストをパラメータとして追加\n",
    "    \n",
    "    # トレーニングタスク\n",
    "    save_task = save_op(model_name=model_name)\n",
    "    save_task.add_pvolumes({'/mnt/saved_model': dsl.PipelineVolume(pvc=model_pvc_name)})\n",
    "\n",
    "    gen_text_task = gen_text_op(input_text=input_text)\n",
    "    gen_text_task.add_pvolumes({\n",
    "        '/mnt/saved_model': dsl.PipelineVolume(pvc=model_pvc_name),\n",
    "        '/mnt/output': dsl.PipelineVolume(pvc=output_pvc_name)\n",
    "    })\n",
    "    gen_text_task.after(save_task)\n",
    "    gen_text_task.outputs['output_text']\n",
    "    \n",
    "# コンパイル\n",
    "kfp.compiler.Compiler().compile(mnist_pipeline, 'llm-pipeline-save-model-and-gen-text.yaml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
